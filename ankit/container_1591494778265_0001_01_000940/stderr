SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/mnt/yarn/usercache/hadoop/filecache/10/__spark_libs__8474873730201561419.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
20/06/06 22:11:31 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 14711@ip-10-128-11-171
20/06/06 22:11:31 INFO SignalUtils: Registered signal handler for TERM
20/06/06 22:11:31 INFO SignalUtils: Registered signal handler for HUP
20/06/06 22:11:31 INFO SignalUtils: Registered signal handler for INT
20/06/06 22:11:32 INFO SecurityManager: Changing view acls to: yarn,hadoop
20/06/06 22:11:32 INFO SecurityManager: Changing modify acls to: yarn,hadoop
20/06/06 22:11:32 INFO SecurityManager: Changing view acls groups to: 
20/06/06 22:11:32 INFO SecurityManager: Changing modify acls groups to: 
20/06/06 22:11:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set()
20/06/06 22:11:32 INFO TransportClientFactory: Successfully created connection to ip-10-128-11-116.us-west-2.compute.internal/10.128.11.116:42673 after 95 ms (0 ms spent in bootstraps)
20/06/06 22:11:33 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
20/06/06 22:11:33 INFO SecurityManager: Changing view acls to: yarn,hadoop
20/06/06 22:11:33 INFO SecurityManager: Changing modify acls to: yarn,hadoop
20/06/06 22:11:33 INFO SecurityManager: Changing view acls groups to: 
20/06/06 22:11:33 INFO SecurityManager: Changing modify acls groups to: 
20/06/06 22:11:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set()
20/06/06 22:11:33 INFO TransportClientFactory: Successfully created connection to ip-10-128-11-116.us-west-2.compute.internal/10.128.11.116:42673 after 1 ms (0 ms spent in bootstraps)
20/06/06 22:11:33 INFO DiskBlockManager: Created local directory at /mnt/yarn/usercache/hadoop/appcache/application_1591494778265_0001/blockmgr-5e490a38-3892-47dd-aa93-8731e14595d6
20/06/06 22:11:33 INFO MemoryStore: MemoryStore started with capacity 9.4 GB
20/06/06 22:11:33 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@ip-10-128-11-116.us-west-2.compute.internal:42673
20/06/06 22:11:33 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
20/06/06 22:11:33 INFO Executor: Starting executor ID 643 on host ip-10-128-11-171.us-west-2.compute.internal
20/06/06 22:11:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37935.
20/06/06 22:11:33 INFO NettyBlockTransferService: Server created on ip-10-128-11-171.us-west-2.compute.internal:37935
20/06/06 22:11:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/06/06 22:11:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(643, ip-10-128-11-171.us-west-2.compute.internal, 37935, None)
20/06/06 22:11:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(643, ip-10-128-11-171.us-west-2.compute.internal, 37935, None)
20/06/06 22:11:33 INFO BlockManager: external shuffle service port = 7337
20/06/06 22:11:33 INFO BlockManager: Registering executor with local external shuffle service.
20/06/06 22:11:33 INFO TransportClientFactory: Successfully created connection to ip-10-128-11-171.us-west-2.compute.internal/10.128.11.171:7337 after 0 ms (0 ms spent in bootstraps)
20/06/06 22:11:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(643, ip-10-128-11-171.us-west-2.compute.internal, 37935, None)
20/06/06 22:11:33 INFO CoarseGrainedExecutorBackend: Got assigned task 8378
20/06/06 22:11:33 INFO CoarseGrainedExecutorBackend: Got assigned task 8379
20/06/06 22:11:33 INFO CoarseGrainedExecutorBackend: Got assigned task 8380
20/06/06 22:11:33 INFO CoarseGrainedExecutorBackend: Got assigned task 8381
20/06/06 22:11:33 INFO Executor: Running task 265.0 in stage 14.9 (TID 8379)
20/06/06 22:11:33 INFO Executor: Running task 267.0 in stage 14.9 (TID 8381)
20/06/06 22:11:33 INFO Executor: Running task 264.0 in stage 14.9 (TID 8378)
20/06/06 22:11:33 INFO Executor: Running task 266.0 in stage 14.9 (TID 8380)
20/06/06 22:11:33 INFO MapOutputTrackerWorker: Updating epoch to 52 and clearing cache
20/06/06 22:11:33 INFO TorrentBroadcast: Started reading broadcast variable 44
20/06/06 22:11:33 INFO TransportClientFactory: Successfully created connection to ip-10-128-11-171.us-west-2.compute.internal/10.128.11.171:36161 after 1 ms (0 ms spent in bootstraps)
20/06/06 22:11:33 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 25.6 KB, free 9.4 GB)
20/06/06 22:11:33 INFO TorrentBroadcast: Reading broadcast variable 44 took 159 ms
20/06/06 22:11:34 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 189.6 KB, free 9.4 GB)
20/06/06 22:11:34 INFO CodeGenerator: Code generated in 295.575734 ms
20/06/06 22:11:34 INFO CodeGenerator: Code generated in 13.191418 ms
20/06/06 22:11:34 INFO CodeGenerator: Code generated in 18.226582 ms
20/06/06 22:11:34 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=10/day=3/000036_0, range: 0-29738, partition values: [2019,10,3]
20/06/06 22:11:34 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=11/000032_0, range: 0-28965, partition values: [2020,2,11]
20/06/06 22:11:34 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=30/000047_0, range: 0-29475, partition values: [2020,3,30]
20/06/06 22:11:34 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=5/day=13/000046_0, range: 0-29867, partition values: [2018,5,13]
20/06/06 22:11:34 INFO TorrentBroadcast: Started reading broadcast variable 15
20/06/06 22:11:34 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 39.2 KB, free 9.4 GB)
20/06/06 22:11:34 INFO TorrentBroadcast: Reading broadcast variable 15 took 8 ms
20/06/06 22:11:34 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 595.0 KB, free 9.4 GB)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt1/s3
java.nio.file.AccessDeniedException: /mnt1
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt1/s3
java.nio.file.AccessDeniedException: /mnt1
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt1/s3
java.nio.file.AccessDeniedException: /mnt1
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt1/s3
java.nio.file.AccessDeniedException: /mnt1
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt2/s3
java.nio.file.AccessDeniedException: /mnt2
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt2/s3
java.nio.file.AccessDeniedException: /mnt2
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt2/s3
java.nio.file.AccessDeniedException: /mnt2
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt3/s3
java.nio.file.AccessDeniedException: /mnt3
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt3/s3
java.nio.file.AccessDeniedException: /mnt3
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt2/s3
java.nio.file.AccessDeniedException: /mnt2
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt4/s3
java.nio.file.AccessDeniedException: /mnt4
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt4/s3
java.nio.file.AccessDeniedException: /mnt4
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt3/s3
java.nio.file.AccessDeniedException: /mnt3
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt5/s3
java.nio.file.AccessDeniedException: /mnt5
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt5/s3
java.nio.file.AccessDeniedException: /mnt5
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt3/s3
java.nio.file.AccessDeniedException: /mnt3
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt6/s3
java.nio.file.AccessDeniedException: /mnt6
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt6/s3
java.nio.file.AccessDeniedException: /mnt6
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt4/s3
java.nio.file.AccessDeniedException: /mnt4
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt7/s3
java.nio.file.AccessDeniedException: /mnt7
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt7/s3
java.nio.file.AccessDeniedException: /mnt7
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt4/s3
java.nio.file.AccessDeniedException: /mnt4
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt8/s3
java.nio.file.AccessDeniedException: /mnt8
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt8/s3
java.nio.file.AccessDeniedException: /mnt8
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt5/s3
java.nio.file.AccessDeniedException: /mnt5
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt9/s3
java.nio.file.AccessDeniedException: /mnt9
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt9/s3
java.nio.file.AccessDeniedException: /mnt9
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt5/s3
java.nio.file.AccessDeniedException: /mnt5
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt10/s3
java.nio.file.AccessDeniedException: /mnt10
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt10/s3
java.nio.file.AccessDeniedException: /mnt10
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt6/s3
java.nio.file.AccessDeniedException: /mnt6
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt11/s3
java.nio.file.AccessDeniedException: /mnt11
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt11/s3
java.nio.file.AccessDeniedException: /mnt11
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt6/s3
java.nio.file.AccessDeniedException: /mnt6
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt7/s3
java.nio.file.AccessDeniedException: /mnt7
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt7/s3
java.nio.file.AccessDeniedException: /mnt7
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt8/s3
java.nio.file.AccessDeniedException: /mnt8
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt8/s3
java.nio.file.AccessDeniedException: /mnt8
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt9/s3
java.nio.file.AccessDeniedException: /mnt9
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt9/s3
java.nio.file.AccessDeniedException: /mnt9
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt10/s3
java.nio.file.AccessDeniedException: /mnt10
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt10/s3
java.nio.file.AccessDeniedException: /mnt10
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt11/s3
java.nio.file.AccessDeniedException: /mnt11
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 WARN ConfigurationUtils: Cannot create temp dir with proper permission: /mnt11/s3
java.nio.file.AccessDeniedException: /mnt11
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
	at java.nio.file.Files.createDirectory(Files.java:674)
	at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
	at java.nio.file.Files.createDirectories(Files.java:767)
	at com.amazon.ws.emr.hadoop.fs.util.ConfigurationUtils.getTestedTempPaths(ConfigurationUtils.java:258)
	at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.<init>(ConsistencyCheckerS3FileSystem.java:160)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.createConsistencyCheckerFileSystem(S3NativeFileSystem2.java:405)
	at com.amazon.ws.emr.hadoop.fs.s3n2.S3NativeFileSystem2.initialize(S3NativeFileSystem2.java:89)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.initialize(EmrFileSystem.java:112)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2859)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2896)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2878)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:392)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:179)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$$anonfun$buildReaderWithPartitionValues$2.apply(OrcFileFormat.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.scan_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/06/06 22:11:37 INFO OrcCodecPool: Got brand-new codec SNAPPY
20/06/06 22:11:37 INFO OrcCodecPool: Got brand-new codec SNAPPY
20/06/06 22:11:37 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=10/day=3/000036_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29738, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:37 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=5/day=13/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29867, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:37 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=30/000047_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29475, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:37 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=11/000032_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28965, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:37 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:37 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:37 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:37 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:37 INFO OrcCodecPool: Got brand-new codec SNAPPY
20/06/06 22:11:37 INFO OrcCodecPool: Got brand-new codec SNAPPY
20/06/06 22:11:39 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=1/day=23/000038_0, range: 0-29736, partition values: [2018,1,23]
20/06/06 22:11:39 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=10/day=4/000043_0, range: 0-28959, partition values: [2018,10,4]
20/06/06 22:11:40 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=10/day=4/000043_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28959, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:40 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:40 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=1/day=23/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29736, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:40 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:40 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=7/day=1/005032_0, range: 0-29472, partition values: [2016,7,1]
20/06/06 22:11:40 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=7/day=1/005032_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29472, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:40 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:11:41 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=5/day=15/005031_0, range: 0-29472, partition values: [2016,5,15]
20/06/06 22:11:41 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=5/day=15/005031_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29472, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:41 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:11:41 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=24/000044_0, range: 0-29864, partition values: [2017,7,24]
20/06/06 22:11:41 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=24/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29864, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:41 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:41 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=2/day=19/000046_0, range: 0-29472, partition values: [2019,2,19]
20/06/06 22:11:42 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=2/day=19/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29472, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:42 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:42 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=2/day=13/000042_0, range: 0-28955, partition values: [2019,2,13]
20/06/06 22:11:43 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=5/day=5/000047_0, range: 0-29857, partition values: [2020,5,5]
20/06/06 22:11:43 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=2/day=13/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28955, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:43 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:43 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=5/day=5/000047_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29857, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:43 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:44 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=10/day=29/000039_0, range: 0-29735, partition values: [2018,10,29]
20/06/06 22:11:44 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=10/day=29/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29735, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:44 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:45 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=7/day=21/000044_0, range: 0-29467, partition values: [2018,7,21]
20/06/06 22:11:45 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=7/day=21/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29467, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:45 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:46 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=23/000046_0, range: 0-29855, partition values: [2017,7,23]
20/06/06 22:11:46 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=10/000041_0, range: 0-28954, partition values: [2018,12,10]
20/06/06 22:11:46 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=23/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29855, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:46 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:47 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=10/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28954, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:47 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:47 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=7/day=31/000044_0, range: 0-29466, partition values: [2018,7,31]
20/06/06 22:11:48 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=7/day=31/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29466, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:48 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:48 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=13/000042_0, range: 0-29851, partition values: [2017,5,13]
20/06/06 22:11:48 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=3/day=26/000042_0, range: 0-29731, partition values: [2018,3,26]
20/06/06 22:11:48 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=3/day=26/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29731, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:48 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:48 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=13/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29851, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:48 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:50 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=2/day=4/000037_0, range: 0-29848, partition values: [2017,2,4]
20/06/06 22:11:50 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=4/day=19/005003_0, range: 0-28952, partition values: [2016,4,19]
20/06/06 22:11:50 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=2/day=4/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29848, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:50 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:11:50 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=4/day=19/005003_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 28952, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:50 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:11:51 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=14/000041_0, range: 0-29466, partition values: [2020,2,14]
20/06/06 22:11:51 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=2/000041_0, range: 0-28950, partition values: [2017,9,2]
20/06/06 22:11:51 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=14/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29466, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:51 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:51 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=4/000036_0, range: 0-29847, partition values: [2020,3,4]
20/06/06 22:11:51 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=2/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28950, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:51 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:51 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=4/000036_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29847, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:51 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:53 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=4/day=6/000044_0, range: 0-29728, partition values: [2018,4,6]
20/06/06 22:11:53 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=10/day=25/000037_0, range: 0-28950, partition values: [2017,10,25]
20/06/06 22:11:53 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=4/day=25/000040_0, range: 0-29465, partition values: [2019,4,25]
20/06/06 22:11:53 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=1/000038_0, range: 0-29846, partition values: [2017,9,1]
20/06/06 22:11:53 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=10/day=25/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28950, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:53 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:53 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=4/day=6/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29728, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:53 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:53 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=4/day=25/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29465, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:53 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:54 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=1/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29846, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:54 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:55 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=12/day=2/000037_0, range: 0-29465, partition values: [2019,12,2]
20/06/06 22:11:55 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=12/day=2/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29465, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:55 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:55 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=10/day=31/005008_0, range: 0-29845, partition values: [2016,10,31]
20/06/06 22:11:56 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=10/day=31/005008_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29845, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:56 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:11:56 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=15/000046_0, range: 0-29845, partition values: [2018,12,15]
20/06/06 22:11:56 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=2/000049_0, range: 0-28950, partition values: [2018,12,2]
20/06/06 22:11:56 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=15/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29845, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:56 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:57 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=2/000049_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28950, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:57 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:57 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=8/000039_0, range: 0-29462, partition values: [2017,11,8]
20/06/06 22:11:57 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=8/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29462, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:57 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:58 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=8/000038_0, range: 0-29726, partition values: [2017,11,8]
20/06/06 22:11:58 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=8/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29726, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:58 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:11:59 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=18/000032_0, range: 0-29836, partition values: [2017,9,18]
20/06/06 22:11:59 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=18/000032_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29836, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:11:59 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:00 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=7/day=28/000044_0, range: 0-28949, partition values: [2018,7,28]
20/06/06 22:12:01 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=7/day=28/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28949, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:01 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:01 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=31/000040_0, range: 0-29716, partition values: [2017,5,31]
20/06/06 22:12:01 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=23/000038_0, range: 0-29457, partition values: [2017,9,23]
20/06/06 22:12:02 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=23/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29457, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:02 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:02 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=31/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29716, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:02 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:02 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=3/day=22/000038_0, range: 0-29835, partition values: [2018,3,22]
20/06/06 22:12:03 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=3/day=22/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29835, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:03 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:03 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=4/day=19/000043_0, range: 0-28945, partition values: [2018,4,19]
20/06/06 22:12:03 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=11/day=10/000044_0, range: 0-29714, partition values: [2018,11,10]
20/06/06 22:12:03 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=4/day=19/000043_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28945, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:03 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:03 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=11/day=10/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29714, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:03 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:04 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=7/day=31/005033_0, range: 0-29456, partition values: [2016,7,31]
20/06/06 22:12:04 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=7/day=31/005033_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29456, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:04 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:04 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=17/000041_0, range: 0-29455, partition values: [2017,9,17]
20/06/06 22:12:05 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=17/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29455, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:05 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:06 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=9/day=21/000045_0, range: 0-28943, partition values: [2019,9,21]
20/06/06 22:12:06 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=9/day=21/000045_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28943, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:06 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:07 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=7/000039_0, range: 0-29835, partition values: [2017,12,7]
20/06/06 22:12:07 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=6/day=19/000041_0, range: 0-29714, partition values: [2018,6,19]
20/06/06 22:12:07 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=7/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29835, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:07 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:07 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=6/day=19/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29714, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:07 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:07 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=2/day=24/005034_0, range: 0-29455, partition values: [2016,2,24]
20/06/06 22:12:07 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=2/day=24/005034_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29455, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:07 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:08 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=16/000040_0, range: 0-28942, partition values: [2017,5,16]
20/06/06 22:12:08 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=4/000041_0, range: 0-29450, partition values: [2017,7,4]
20/06/06 22:12:08 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=16/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28942, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:08 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:08 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=4/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29450, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:08 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:10 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=14/000040_0, range: 0-29450, partition values: [2020,2,14]
20/06/06 22:12:10 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=10/000044_0, range: 0-28935, partition values: [2017,6,10]
20/06/06 22:12:10 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=14/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29450, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:10 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:10 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=10/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28935, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:10 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:11 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=1/day=2/000038_0, range: 0-29713, partition values: [2020,1,2]
20/06/06 22:12:11 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=1/day=2/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29713, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:11 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:12 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=7/000036_0, range: 0-28934, partition values: [2017,9,7]
20/06/06 22:12:12 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=12/day=12/005009_0, range: 0-29833, partition values: [2016,12,12]
20/06/06 22:12:12 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=7/000036_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28934, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:12 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:12 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=12/day=12/005009_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29833, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:12 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:12 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=24/000041_0, range: 0-29449, partition values: [2017,11,24]
20/06/06 22:12:13 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=1/day=3/005010_0, range: 0-29828, partition values: [2017,1,3]
20/06/06 22:12:13 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=24/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29449, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:13 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:13 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=1/day=3/005010_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29828, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:13 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:13 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=9/000046_0, range: 0-29709, partition values: [2018,2,9]
20/06/06 22:12:13 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=9/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29709, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:13 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:14 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=13/000041_0, range: 0-29827, partition values: [2017,5,13]
20/06/06 22:12:14 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=13/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29827, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:14 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:14 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=25/000037_0, range: 0-28933, partition values: [2020,2,25]
20/06/06 22:12:15 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=25/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28933, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:15 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:15 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=9/day=7/005011_0, range: 0-29825, partition values: [2016,9,7]
20/06/06 22:12:16 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=9/day=7/005011_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29825, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:16 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:16 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=27/000042_0, range: 0-29449, partition values: [2018,2,27]
20/06/06 22:12:16 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=10/day=6/000038_0, range: 0-29824, partition values: [2017,10,6]
20/06/06 22:12:16 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=9/000041_0, range: 0-29707, partition values: [2017,6,9]
20/06/06 22:12:16 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=10/day=6/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29824, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:16 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:17 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=27/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29449, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:17 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:17 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=9/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29707, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:17 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:17 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=10/000042_0, range: 0-28932, partition values: [2018,12,10]
20/06/06 22:12:17 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=10/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28932, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:17 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:18 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=2/day=20/000037_0, range: 0-29705, partition values: [2017,2,20]
20/06/06 22:12:19 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=2/day=20/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29705, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:19 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:20 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=2/day=4/000038_0, range: 0-29704, partition values: [2017,2,4]
20/06/06 22:12:20 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=15/000041_0, range: 0-29822, partition values: [2017,11,15]
20/06/06 22:12:20 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=2/day=4/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29704, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:20 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:20 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=15/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29822, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:20 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:20 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=30/000039_0, range: 0-28931, partition values: [2017,5,30]
20/06/06 22:12:21 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=10/day=2/000033_0, range: 0-29445, partition values: [2019,10,2]
20/06/06 22:12:21 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=30/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28931, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:21 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:21 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=10/day=2/000033_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29445, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:21 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:21 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=3/day=13/000041_0, range: 0-29700, partition values: [2019,3,13]
20/06/06 22:12:22 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=3/day=13/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29700, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:22 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:22 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=14/000040_0, range: 0-28928, partition values: [2018,2,14]
20/06/06 22:12:23 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=14/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28928, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:23 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:23 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=15/000038_0, range: 0-29444, partition values: [2017,5,15]
20/06/06 22:12:23 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=15/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29444, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:23 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:24 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=12/day=27/000040_0, range: 0-29700, partition values: [2019,12,27]
20/06/06 22:12:24 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=12/day=27/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29700, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:24 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:24 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=4/day=18/000045_0, range: 0-29822, partition values: [2020,4,18]
20/06/06 22:12:25 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=4/day=18/000045_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29822, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:25 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:26 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=24/000041_0, range: 0-29698, partition values: [2017,12,24]
20/06/06 22:12:26 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=10/day=8/000042_0, range: 0-29441, partition values: [2017,10,8]
20/06/06 22:12:26 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=24/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29698, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:26 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:26 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=4/day=27/000046_0, range: 0-29822, partition values: [2018,4,27]
20/06/06 22:12:27 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=10/day=8/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29441, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:27 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:27 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=4/day=27/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29822, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:27 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:27 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=1/000039_0, range: 0-28924, partition values: [2017,9,1]
20/06/06 22:12:28 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=1/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28924, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:28 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:30 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=7/005035_0, range: 0-29437, partition values: [2016,1,7]
20/06/06 22:12:30 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=30/000041_0, range: 0-28918, partition values: [2019,5,30]
20/06/06 22:12:30 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=7/005035_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29437, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:30 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:30 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=30/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28918, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:30 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:30 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=14/000044_0, range: 0-29434, partition values: [2018,12,14]
20/06/06 22:12:30 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=31/000041_0, range: 0-29820, partition values: [2018,12,31]
20/06/06 22:12:30 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=9/day=14/000040_0, range: 0-29697, partition values: [2018,9,14]
20/06/06 22:12:31 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=14/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29434, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:31 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:31 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=12/day=31/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29820, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:31 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:31 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=9/day=14/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29697, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:31 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:32 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=2/000037_0, range: 0-28917, partition values: [2017,5,2]
20/06/06 22:12:32 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=2/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28917, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:32 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:33 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=11/day=27/000042_0, range: 0-29695, partition values: [2019,11,27]
20/06/06 22:12:33 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=9/day=23/000044_0, range: 0-28916, partition values: [2018,9,23]
20/06/06 22:12:34 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=6/day=25/000045_0, range: 0-29432, partition values: [2018,6,25]
20/06/06 22:12:34 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=9/day=23/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28916, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:34 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:34 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=11/day=27/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29695, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:34 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:34 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=8/day=9/000045_0, range: 0-29819, partition values: [2017,8,9]
20/06/06 22:12:34 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=6/day=25/000045_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29432, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:34 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:34 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=8/day=9/000045_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29819, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:34 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:36 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=10/000044_0, range: 0-29812, partition values: [2017,7,10]
20/06/06 22:12:36 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=21/000040_0, range: 0-29695, partition values: [2017,5,21]
20/06/06 22:12:36 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=7/day=10/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29812, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:36 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:36 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=5/day=21/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29695, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:36 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:37 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=3/day=18/000037_0, range: 0-28909, partition values: [2017,3,18]
20/06/06 22:12:37 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=3/day=23/000042_0, range: 0-29431, partition values: [2019,3,23]
20/06/06 22:12:37 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=3/day=18/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 28909, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:37 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:37 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=3/day=23/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29431, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:37 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:38 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=1/000036_0, range: 0-29811, partition values: [2017,6,1]
20/06/06 22:12:38 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=1/day=17/000033_0, range: 0-29693, partition values: [2017,1,17]
20/06/06 22:12:38 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=1/000036_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29811, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:38 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:38 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=1/day=17/000033_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29693, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:38 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:38 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=1/day=3/005055_0, range: 0-28906, partition values: [2017,1,3]
20/06/06 22:12:39 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=1/day=3/005055_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 28906, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:39 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:39 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=25/000040_0, range: 0-29428, partition values: [2019,5,25]
20/06/06 22:12:39 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=25/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29428, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:39 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:39 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=3/day=30/000033_0, range: 0-28903, partition values: [2017,3,30]
20/06/06 22:12:39 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=15/000046_0, range: 0-29809, partition values: [2020,3,15]
20/06/06 22:12:40 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=1/day=3/000045_0, range: 0-29690, partition values: [2020,1,3]
20/06/06 22:12:40 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=3/day=30/000033_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 28903, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:40 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:40 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=1/day=3/000045_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29690, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:40 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:40 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=15/000046_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29809, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:40 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:40 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=1/day=31/000040_0, range: 0-29426, partition values: [2019,1,31]
20/06/06 22:12:41 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=1/day=31/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29426, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:41 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:41 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=4/day=17/000043_0, range: 0-28902, partition values: [2019,4,17]
20/06/06 22:12:41 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=2/005012_0, range: 0-29803, partition values: [2016,1,2]
20/06/06 22:12:41 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=4/day=17/000043_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28902, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:41 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:41 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=10/day=16/000039_0, range: 0-29689, partition values: [2019,10,16]
20/06/06 22:12:41 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=2/005012_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29803, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:41 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:42 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=10/day=16/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29689, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:42 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:42 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=25/000041_0, range: 0-29802, partition values: [2017,12,25]
20/06/06 22:12:42 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=25/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29802, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:42 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:42 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=19/000035_0, range: 0-28901, partition values: [2017,9,19]
20/06/06 22:12:43 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=9/day=19/000035_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28901, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:43 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:43 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=8/day=27/000038_0, range: 0-29689, partition values: [2017,8,27]
20/06/06 22:12:44 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=8/day=27/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29689, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:44 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:45 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2015/month=12/day=28/005036_0, range: 0-29424, partition values: [2015,12,28]
20/06/06 22:12:45 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2015/month=12/day=28/005036_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29424, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:45 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:45 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=11/day=14/000032_0, range: 0-29422, partition values: [2019,11,14]
20/06/06 22:12:46 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=11/day=14/000032_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29422, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:46 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:46 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=11/000048_0, range: 0-28898, partition values: [2018,2,11]
20/06/06 22:12:46 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=2/day=11/000048_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28898, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:46 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:46 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=31/000048_0, range: 0-29686, partition values: [2020,3,31]
20/06/06 22:12:46 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=6/day=1/000041_0, range: 0-29798, partition values: [2019,6,1]
20/06/06 22:12:46 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=6/day=1/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29798, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:46 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:47 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=31/000048_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29686, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:47 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:48 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=8/day=3/000043_0, range: 0-29797, partition values: [2018,8,3]
20/06/06 22:12:48 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=1/day=12/000040_0, range: 0-29420, partition values: [2018,1,12]
20/06/06 22:12:48 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=8/day=3/000043_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29797, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:48 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:48 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=1/day=12/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29420, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:48 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:49 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=23/000039_0, range: 0-29685, partition values: [2019,5,23]
20/06/06 22:12:49 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=23/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29685, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:49 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:49 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=17/000041_0, range: 0-28895, partition values: [2017,11,17]
20/06/06 22:12:49 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=11/day=17/000041_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28895, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:49 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:50 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=6/day=7/005019_0, range: 0-29684, partition values: [2016,6,7]
20/06/06 22:12:50 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=6/day=7/005019_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29684, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:50 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:51 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=3/day=9/000044_0, range: 0-29679, partition values: [2018,3,9]
20/06/06 22:12:51 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2018/month=3/day=9/000044_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29679, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:51 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:51 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=8/day=3/000043_0, range: 0-29797, partition values: [2019,8,3]
20/06/06 22:12:52 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=8/day=3/000043_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29797, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:52 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:52 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=30/000040_0, range: 0-29419, partition values: [2017,12,30]
20/06/06 22:12:52 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=12/day=30/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29419, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:52 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:52 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=9/day=4/000036_0, range: 0-28894, partition values: [2019,9,4]
20/06/06 22:12:52 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=9/day=4/000036_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28894, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:52 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:53 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=5/005013_0, range: 0-29792, partition values: [2016,1,5]
20/06/06 22:12:54 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=5/005013_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29792, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:54 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:54 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=26/000043_0, range: 0-28894, partition values: [2019,5,26]
20/06/06 22:12:54 INFO CodeGenerator: Code generated in 10.086837 ms
20/06/06 22:12:54 INFO CodeGenerator: Code generated in 10.888947 ms
20/06/06 22:12:54 INFO CodeGenerator: Code generated in 6.518984 ms
20/06/06 22:12:54 INFO CodeGenerator: Code generated in 7.28457 ms
20/06/06 22:12:54 INFO CodeGenerator: Code generated in 12.97605 ms
20/06/06 22:12:54 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=5/day=26/000043_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28894, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:54 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:54 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=4/000040_0, range: 0-29679, partition values: [2017,6,4]
20/06/06 22:12:54 INFO Executor: Finished task 264.0 in stage 14.9 (TID 8378). 2550 bytes result sent to driver
20/06/06 22:12:54 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2017/month=6/day=4/000040_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29679, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:54 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:55 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=21/005037_0, range: 0-29418, partition values: [2016,1,21]
20/06/06 22:12:55 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2016/month=1/day=21/005037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false], offset: 0, length: 29418, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:55 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string>
20/06/06 22:12:56 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=4/000039_0, range: 0-28892, partition values: [2020,2,4]
20/06/06 22:12:56 INFO Executor: Finished task 266.0 in stage 14.9 (TID 8380). 2550 bytes result sent to driver
20/06/06 22:12:56 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=2/day=4/000039_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 28892, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:56 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:56 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=12/000038_0, range: 0-29678, partition values: [2020,3,12]
20/06/06 22:12:56 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2020/month=3/day=12/000038_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29678, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:56 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:12:58 INFO Executor: Finished task 267.0 in stage 14.9 (TID 8381). 2593 bytes result sent to driver
20/06/06 22:12:58 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=6/day=12/000042_0, range: 0-29676, partition values: [2019,6,12]
20/06/06 22:12:58 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=6/day=12/000042_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29676, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:12:58 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:13:00 INFO FileScanRDD: Reading File path: s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=9/day=27/000037_0, range: 0-29676, partition values: [2019,9,27]
20/06/06 22:13:00 INFO ReaderImpl: Reading ORC rows from s3://imvudata/user/hive/warehouse/events_etl.db/event_ui_imvu_account_events/year=2019/month=9/day=27/000037_0 with {include: [true, false, true, false, false, true, false, false, false, false, false, true, false, true], offset: 0, length: 29676, sarg: leaf-0 = (IS_NULL customers_id), expr = (not leaf-0), columns: ['event_time', 'customers_id', 'client_session', 'user_agent', 'user_agent_platform', 'url', 'email', 'dob', 'country', 'avatarname', 'reg_token_type', 'is_internal_ip', 'registration_flow'], includeAcidColumns: true}
20/06/06 22:13:00 INFO RecordReaderImpl: Reader schema not provided -- using file schema struct<_col0:string,_col1:string,_col2:string,_col3:string,_col4:string,_col5:string,_col6:string,_col7:string,_col8:string,_col9:string,_col10:string,_col11:string,_col12:string>
20/06/06 22:13:02 INFO Executor: Finished task 265.0 in stage 14.9 (TID 8379). 2550 bytes result sent to driver
20/06/06 22:14:06 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/06/06 22:14:06 INFO DiskBlockManager: Shutdown hook called
20/06/06 22:14:06 INFO ShutdownHookManager: Shutdown hook called
